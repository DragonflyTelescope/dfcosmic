
@misc{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	shorttitle = {{PyTorch}},
	url = {https://ui.adsabs.harvard.edu/abs/2019arXiv191201703P},
	doi = {10.48550/arXiv.1912.01703},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
	urldate = {2025-12-08},
	publisher = {arXiv},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Köpf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	month = dec,
	year = {2019},
	note = {ADS Bibcode: 2019arXiv191201703P},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Mathematical Software},
	file = {Full Text PDF:/home/carterrhea/snap/zotero-snap/common/Zotero/storage/7AQRRHBX/Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Deep Learning Library.pdf:application/pdf},
}

@misc{robitaille_astropyastroscrappy_2025,
	title = {astropy/astroscrappy: v1.3.0},
	shorttitle = {astropy/astroscrappy},
	url = {https://zenodo.org/records/17495347},
	urldate = {2025-12-08},
	publisher = {Zenodo},
	author = {Robitaille, Thomas and Conseil, Simon and Sipőcz, Brigitta and McCully, Curtis and Tollerud, Erik and Droettboom, Michael and Lim, P. L. and Bradley, Larry and Bray, E. M. and Craig, Matt and Deil, Christoph and Price-Whelan, Adrian and Barbary, Kyle and Ginsburg, Adam and Robert, Clément and Kerzendorf, Wolfgang and Gupta, Ayush and D'Avella, Dan and Burke, Doug and Betts, Edward and Crawford, Steve and Zabalza, Víctor and Günther, Hans Moritz},
	month = oct,
	year = {2025},
	doi = {10.5281/zenodo.17495347},
	file = {Snapshot:/home/carterrhea/snap/zotero-snap/common/Zotero/storage/R8D23I55/17495347.html:text/html},
}

@misc{bradley_larrybradleylacosmic_2025,
	title = {larrybradley/lacosmic: 1.3.0},
	shorttitle = {larrybradley/lacosmic},
	url = {https://zenodo.org/records/15831925},
	abstract = {See the changelog for release notes.},
	urldate = {2025-12-08},
	publisher = {Zenodo},
	author = {Bradley, Larry},
	month = jul,
	year = {2025},
	doi = {10.5281/zenodo.15831925},
	file = {Snapshot:/home/carterrhea/snap/zotero-snap/common/Zotero/storage/ZL3JT4RP/15831925.html:text/html},
}

@article{van_dokkum_cosmic-ray_2001,
	title = {Cosmic-{Ray} {Rejection} by {Laplacian} {Edge} {Detection}},
	volume = {113},
	issn = {0004-6280},
	url = {https://ui.adsabs.harvard.edu/abs/2001PASP..113.1420V},
	doi = {10.1086/323894},
	abstract = {Conventional algorithms for rejecting cosmic rays in single CCD exposures rely on the contrast between cosmic rays and their surroundings and may produce erroneous results if the point-spread function is smaller than the largest cosmic rays. This paper describes a robust algorithm for cosmic-ray rejection, based on a variation of Laplacian edge detection. The algorithm identifies cosmic rays of arbitrary shapes and sizes by the sharpness of their edges and reliably discriminates between poorly sampled point sources and cosmic rays. Examples of its performance are given for spectroscopic and imaging data, including Hubble Space Telescope Wide Field Planetary Camera 2 images.},
	urldate = {2025-12-08},
	journal = {Publications of the Astronomical Society of the Pacific},
	author = {van Dokkum, Pieter G.},
	month = nov,
	year = {2001},
	note = {Publisher: IOP
ADS Bibcode: 2001PASP..113.1420V},
	keywords = {Astrophysics, Instrumentation: Detectors, Methods: Data Analysis-techniques: image processing},
	pages = {1420--1427},
	file = {Full Text PDF:/home/carterrhea/snap/zotero-snap/common/Zotero/storage/FEBJWVGY/van Dokkum - 2001 - Cosmic-Ray Rejection by Laplacian Edge Detection.pdf:application/pdf},
}

@misc{crameri_scientific_2023,
	title = {Scientific colour maps},
	url = {https://zenodo.org/records/8409685},
	abstract = {Suite of scientific, colour-vision deficiency friendly and perceptually-uniform colour maps (www.fabiocrameri.ch/colourmaps) that include all readers and significantly reduce visual errors.



	
Book graphic design masterclasses on how to best use colour in a scientific context via www.fabiocrameri.ch/masterclasses.
	
Commission professional graphic design support via Undertone.design.
	
Support the underlying cause and the development of the Scientific colour maps via www.fabiocrameri.ch/products.},
	urldate = {2026-01-26},
	publisher = {Zenodo},
	author = {Crameri, Fabio},
	month = oct,
	year = {2023},
	doi = {10.5281/zenodo.8409685},
	note = {Language: eng},
	keywords = {batlow, color gradient, color map, color palette, color perception, color scheme, colormap, colour palettes, colour schemes, colour vision deficiency, CVD, data representation, perceptually uniform, scientific colormap, scientific colour maps, scientific colourmap, scientific visualisation, visualisation, visualization},
	file = {Snapshot:/home/carterrhea/snap/zotero-snap/common/Zotero/storage/EXCF84M6/8409685.html:text/html},
}

@article{zhang_deepcr_2020,
	title = {{deepCR}: {Cosmic} {Ray} {Rejection} with {Deep} {Learning}},
	volume = {889},
	issn = {0004-637X},
	shorttitle = {{deepCR}},
	url = {https://doi.org/10.3847/1538-4357/ab3fa6},
	doi = {10.3847/1538-4357/ab3fa6},
	abstract = {Cosmic ray (CR) identification and replacement are critical components of imaging and spectroscopic reduction pipelines involving solid-state detectors. We present deepCR, a deep-learning-based framework for CR identification and subsequent image inpainting based on the predicted CR mask. To demonstrate the effectiveness of this framework, we train and evaluate models on Hubble Space Telescope (HST) ACS/WFC images of sparse extragalactic fields, globular clusters, and resolved galaxies. We demonstrate that at a false-positive rate of 0.5\%, deepCR achieves close to 100\% detection rates in both extragalactic and globular cluster fields, and 91\% in resolved galaxy fields, which is a significant improvement over the current state-of-the-art method LACosmic. Compared with a multicore CPU implementation of LACosmic, deepCR CR mask predictions run up to 6.5 times faster on a CPU and 90 times faster on a single GPU. For image inpainting, the mean squared errors of deepCR predictions are 20 times lower in globular cluster fields, 5 times lower in resolved galaxy fields, and 2.5 times lower in extragalactic fields, compared with the best performing nonneural technique tested. We present our framework and the trained models as an open-source Python project , with a simple-to-use API. To facilitate reproducibility of the results we also provide a benchmarking codebase .},
	language = {en},
	number = {1},
	urldate = {2026-01-27},
	journal = {ApJ},
	author = {Zhang, Keming and Bloom, Joshua S.},
	month = jan,
	year = {2020},
	note = {Publisher: The American Astronomical Society},
	pages = {24},
	file = {IOP Full Text PDF:/home/carterrhea/snap/zotero-snap/common/Zotero/storage/MYCHLFFY/Zhang and Bloom - 2020 - deepCR Cosmic Ray Rejection with Deep Learning.pdf:application/pdf},
}

@article{pych_fast_2003,
	title = {A {Fast} {Algorithm} for {Cosmic}‐{Ray} {Removal} from {Single} {Images}},
	volume = {116},
	issn = {1538-3873},
	url = {https://doi.org/10.1086/381786},
	doi = {10.1086/381786},
	abstract = {We present a method for detecting cosmic rays in single images. The algorithm is based on a simple analysis of the histogram of the image data and does not use any modeling of the picture of the object. It does not require a good signal‐to‐noise ratio in the image data. Identification of multiple‐pixel cosmic‐ray hits is realized by running the procedure for detection and replacement iteratively. The tests performed by us show that the method is very effective when applied to the images with spectroscopic data. It is also very fast in comparison with other single‐image algorithms found in astronomical data‐processing packages. Practical implementation and examples of application are presented.},
	language = {en},
	number = {816},
	urldate = {2026-01-27},
	journal = {PASP},
	author = {Pych, Wojtek},
	month = dec,
	year = {2003},
	note = {Publisher: The University of Chicago Press},
	pages = {148},
	file = {IOP Full Text PDF:/home/carterrhea/snap/zotero-snap/common/Zotero/storage/D348QK74/Pych - 2003 - A Fast Algorithm for Cosmic‐Ray Removal from Single Images.pdf:application/pdf},
}

@article{xu__cosmic-conn_2023,
	title = {Cosmic-{CoNN}: {A} {Cosmic}-{Ray} {Detection} {Deep}-learning {Framework}, {Data} {Set}, and {Toolkit}},
	volume = {942},
	issn = {0004-637X, 1538-4357},
	shorttitle = {Cosmic-{CoNN}},
	url = {https://iopscience.iop.org/article/10.3847/1538-4357/ac9d91},
	doi = {10.3847/1538-4357/ac9d91},
	abstract = {Rejecting cosmic rays (CRs) is essential for the scientiﬁc interpretation of CCD-captured data, but detecting CRs in single-exposure images has remained challenging. Conventional CR detectors require experimental parameter tuning for different instruments, and recent deep-learning methods only produce instrument-speciﬁc models that suffer from performance loss on telescopes not included in the training data. We present Cosmic-CoNN, a generic CR detector deployed for 24 telescopes at the Las Cumbres Observatory, which has been made possible by the three contributions in this work: (1) We build a large and diverse ground-based CR data set leveraging thousands of images from a global telescope network. (2) We propose a novel loss function and a neural network optimized for telescope imaging data to train generic CR-detection models. At 95\% recall, our model achieves a precision of 93.70\% on Las Cumbres imaging data and maintains a consistent performance on new ground-based instruments never used for training. Speciﬁcally, the Cosmic-CoNN model trained on the Las Cumbres CR data set maintains high precisions of 92.03\% and 96.69\% on Gemini GMOS-N/S 1 × 1 and 2 × 2 binning images, respectively. (3) We build a suite of tools including an interactive CR mask visualization and editing interface, console commands, and Python APIs to make automatic, robust CR detection widely accessible by the community of astronomers. Our data set, open-source code base, and trained models are available at https://github.com/cy-xu/ cosmic-conn.},
	language = {en},
	number = {2},
	urldate = {2026-01-27},
	journal = {ApJ},
	author = {Xu 许, Chengyuan 程远 and McCully, Curtis and Dong 董, Boning 泊宁 and Howell, D. Andrew and Sen, Pradeep},
	month = jan,
	year = {2023},
	pages = {73},
	file = {PDF:/home/carterrhea/snap/zotero-snap/common/Zotero/storage/229ZNLER/Xu 许 et al. - 2023 - Cosmic-CoNN A Cosmic-Ray Detection Deep-learning Framework, Data Set, and Toolkit.pdf:application/pdf},
}
